<!DOCTYPE html>
<html>

<head>
  <meta name="google-site-verification" content="QPBDjBshVKPivIRMQQwDWikbMtLBLr3S-wzAZq4ZXeg" />
  <meta charset="utf-8">
  <meta name="description" content="SegNet4D">
  <meta name="keywords" content="4D Semantic Segmentation, Moving Object Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SegNet4D</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SegNet4D </h1>
            <h1 class="title is-2 publication-title">Efficient Instance-Aware 4D LiDAR Semantic Segmentation for Driving Scenarios</h1>

            <!-- <div class="column is-full_width"> -->
            <!--   <h2 class="title is-4">conference</h2> -->
            <!-- </div> -->

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/neng-wang/">Neng Wang</a></span>,
              <span class="author-block">
                Ruibin Guo</a></span>,
              </span>
              <span class="author-block">
                <a href="https://github.com/chenghao-shi">Chenghao Shi</a></span>,
              </span> 
              <span class="author-block">
                Ziyue Wang</a></span>,
              </span>  
              <span class="author-block">
                Hui Zhang</a></span>
              </span>
              <span class="author-block">
                <a href="https://www.researchgate.net/profile/Huimin-Lu-3">Huimin Lu</a></span>,
              </span> 
              <span class="author-block">
                Zhiqiang Zheng</a></span>
              </span>
              <span class="author-block">
                <a href="https://www.researchgate.net/profile/Xieyuanli-Chen/research">Xieyuanli Chen</a></span>,
              </span> 
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">National University of Defense Technology, Changsha, China.</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- PDF Link. -->
                <span class="link-block">
                  <a href=" " class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/nubot-nudt/SegNet4D"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              </div>

            </div>
          </div>
        </div>


      </div>
    </div>

  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img class="img-fluid" src="./static/videos/demo.gif" alt="SegNet4D Demo" style="width: 100%;"">
        <!-- <video autoplay muted loop style="width: 100%;">
          <source src="https://omnomnom.vision.rwth-aachen.de/data/interactive4d/teaser_video.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video> -->
        <!-- <br><br> -->
        <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- </h2> -->
      </div>
    </div>
  </section>

  <section class="section">
    
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              <b>TL;DR: 
                4D LiDAR semantic segmentation, also referred to as multi-scan semantic segmentation, 
                plays a crucial role in enhancing the environmental understanding capabilities of autonomous robots. 
                It classifies the semantic category of each LiDAR point and detects whether it is dynamic, a critical ability for tasks like obstacle avoidance and autonomous navigation. 
              </b>
            </p>
            <p>
              Existing approaches often rely on computationally heavy 4D convolutions or recursive networks, 
              which result in poor real-time performance, making them unsuitable for online robotics and autonomous driving applications.
            </p>  
            <p>
              In this paper, we introduce SegNet4D, a novel real-time 4D semantic segmentation network offering both efficiency and strong semantic understanding. 
              SegNet4D addresses 4D segmentation as two tasks: single-scan semantic segmentation and moving object segmentation, each tackled by a separate network head. 
              Both results are combined in a motion-semantic fusion module to achieve comprehensive 4D segmentation.
              Additionally, instance information is extracted from the current scan and exploited for instance-wise segmentation consistency. 
            </p>
            <p>
              Our approach surpasses state-of-the-art in both multi-scan semantic segmentation and moving object segmentation while offering greater efficiency, enabling real-time operation.
              Besides, its effectiveness and efficiency have also been validated on a real-world robotic platform.  <br />
                <!-- We will publicly release code and models upon acceptance. -->
            </p>
                
          </div>
        </div>
      </div>
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <video controls poster="./static/images/cover.jpg">
              <source src="https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL2YvYy81ZjA2YWI0ZWNjYWQ4YTRlL0VqUWVjZTgtbFZoTWtKTDVIOFFyNk5NQlptbGN6WUJ3R01xd0VfSi1iTkpGTEE%5FZT1xc2k2cjA&cid=5F06AB4ECCAD8A4E&id=5F06AB4ECCAD8A4E%21sc3b620f34a6041b2a77e8d6d18d1968f&parId=5F06AB4ECCAD8A4E%21sef711e34953e4c589092f91fc42be8d3&o=OneUp"
                type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      @article{wang2024segnet4d,
        title     = {{SegNet4D: Efficient Instance-Aware 4D LiDAR Semantic Segmentation for Driving Scenarios}},
        author    = {Neng, Wang and Ruibin, Guo and Chenghao, Shi and Ziyue, Wang and Hui, Zhang and Huimin, Lu and Zhiqiang, Zheng and Xieyuanali, Chen},
        journal   = {arXiv},
        year      = {2024}
      }
      </code></pre>
    </div>
  </section>

  <!-- <section class="section" id="Acknowledgment">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgment</h2>
      We thank Yuanwen Yue, Daan de Geus, and Alexander Hermans for their helpful feedback and discussions. We also thank all our annotators who participated in the user study. Theodora Kontogianni is a postdoctoral research fellow at the ETH AI Center and her research is partially funded by the Hasler Stiftung Grant project (23069). Idil Esen Zulfikarâ€™s research is funded by the BMBF project NeuroSys-D (03ZU1106DA). Kadir Yilmaz's research is funded by the Bosch-RWTH LHC project Context Understanding for Autonomous Systems. The computing resources for most of the experiments were granted by the Gauss Centre for Supercomputing e.V. through the John von Neumann Institute for Computing on the GCS Supercomputer JUWELS at Julich Supercomputing Centre.    </div>
  </section> -->

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/YilmazKadir" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
              We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this
              template.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>